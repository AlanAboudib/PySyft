{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7 - Federated Learning with FederatedDataset\n",
    "\n",
    "Here we introduce a new tool for using federated datasets. We have created a `FederatedDataset` class which is intended to be used like the PyTorch Dataset class, and is given to a federated data loader `FederatedDataLoader` which will iterate on it in a federated fashion.\n",
    "\n",
    "\n",
    "Authors:\n",
    "- Andrew Trask - Twitter: [@iamtrask](https://twitter.com/iamtrask)\n",
    "- Th√©o Ryffel - GitHub: [@LaRiffle](https://github.com/LaRiffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the sandbox that we discovered last lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atrask/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/atrask/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/atrask/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/atrask/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Sandbox...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import syft as sy\n",
    "sy.create_sandbox(globals(), verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then search for a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_data = grid.search(\"#boston\", \"#data\", verbose=False, return_counter=False)\n",
    "boston_target = grid.search(\"#boston\", \"#target\", verbose=False, return_counter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a model and an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = boston_data['alice'][0].shape[1]\n",
    "n_targets = 1\n",
    "model = th.nn.Linear(n_features, n_targets)\n",
    "optimizer = th.optim.SGD(params=model.parameters(),lr=0.0000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we cast the data fetched in a `FederatedDataset`. See the workers which hold part of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jon', 'alice', 'theo', 'jason', 'bob', 'andy']\n"
     ]
    }
   ],
   "source": [
    "dataset = sy.FederatedDataset(boston_data, boston_target)\n",
    "print(dataset.workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put it in a `FederatedDataLoader` and specify options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = sy.FederatedDataLoader(dataset, batch_size=4, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we iterate over epochs. You can see how similar this is compared to pure and local PyTorch training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/506 (0%)]\tLoss: 257324.187500\n",
      "Train Epoch: 1 [80/506 (16%)]\tLoss: 164.807587\n",
      "Train Epoch: 1 [80/506 (31%)]\tLoss: 50.742851\n",
      "Train Epoch: 1 [120/506 (47%)]\tLoss: 265.615936\n",
      "Train Epoch: 1 [160/506 (63%)]\tLoss: 977.800781\n",
      "Train Epoch: 1 [200/506 (79%)]\tLoss: 37.221901\n",
      "Train Epoch: 1 [240/506 (94%)]\tLoss: 36.468460\n",
      "Total loss 650352.809401989\n",
      "Train Epoch: 2 [0/506 (0%)]\tLoss: 409.260925\n",
      "Train Epoch: 2 [40/506 (16%)]\tLoss: 57.283073\n",
      "Train Epoch: 2 [80/506 (31%)]\tLoss: 39.606285\n",
      "Train Epoch: 2 [120/506 (47%)]\tLoss: 164.271698\n",
      "Train Epoch: 2 [160/506 (63%)]\tLoss: 941.646362\n",
      "Train Epoch: 2 [200/506 (79%)]\tLoss: 30.483576\n",
      "Train Epoch: 2 [240/506 (94%)]\tLoss: 29.300154\n",
      "Total loss 49260.60708284378\n",
      "Train Epoch: 3 [0/506 (0%)]\tLoss: 195.616425\n",
      "Train Epoch: 3 [40/506 (16%)]\tLoss: 127.807770\n",
      "Train Epoch: 3 [80/506 (31%)]\tLoss: 31.930153\n",
      "Train Epoch: 3 [120/506 (47%)]\tLoss: 121.225540\n",
      "Train Epoch: 3 [160/506 (63%)]\tLoss: 915.360107\n",
      "Train Epoch: 3 [200/506 (79%)]\tLoss: 27.724627\n",
      "Train Epoch: 3 [240/506 (94%)]\tLoss: 26.227091\n",
      "Total loss 44984.10601878166\n",
      "Train Epoch: 4 [0/506 (0%)]\tLoss: 116.847809\n",
      "Train Epoch: 4 [40/506 (16%)]\tLoss: 177.848450\n",
      "Train Epoch: 4 [80/506 (31%)]\tLoss: 26.248579\n",
      "Train Epoch: 4 [120/506 (47%)]\tLoss: 100.872528\n",
      "Train Epoch: 4 [160/506 (63%)]\tLoss: 894.592346\n",
      "Train Epoch: 4 [200/506 (79%)]\tLoss: 26.586819\n",
      "Train Epoch: 4 [240/506 (94%)]\tLoss: 24.773544\n",
      "Total loss 43509.80646944046\n",
      "Train Epoch: 5 [0/506 (0%)]\tLoss: 84.642685\n",
      "Train Epoch: 5 [40/506 (16%)]\tLoss: 203.201065\n",
      "Train Epoch: 5 [80/506 (31%)]\tLoss: 21.761131\n",
      "Train Epoch: 5 [120/506 (47%)]\tLoss: 90.248123\n",
      "Train Epoch: 5 [160/506 (63%)]\tLoss: 877.122070\n",
      "Train Epoch: 5 [200/506 (79%)]\tLoss: 26.219065\n",
      "Train Epoch: 5 [240/506 (94%)]\tLoss: 24.026848\n",
      "Total loss 42793.33108711243\n",
      "Train Epoch: 6 [0/506 (0%)]\tLoss: 70.357437\n",
      "Train Epoch: 6 [40/506 (16%)]\tLoss: 212.634064\n",
      "Train Epoch: 6 [80/506 (31%)]\tLoss: 18.074522\n",
      "Train Epoch: 6 [120/506 (47%)]\tLoss: 84.127205\n",
      "Train Epoch: 6 [160/506 (63%)]\tLoss: 861.790955\n",
      "Train Epoch: 6 [200/506 (79%)]\tLoss: 26.288670\n",
      "Train Epoch: 6 [240/506 (94%)]\tLoss: 23.614159\n",
      "Total loss 42302.21308040619\n",
      "Train Epoch: 7 [0/506 (0%)]\tLoss: 63.703636\n",
      "Train Epoch: 7 [40/506 (16%)]\tLoss: 213.400970\n",
      "Train Epoch: 7 [80/506 (31%)]\tLoss: 14.979881\n",
      "Train Epoch: 7 [120/506 (47%)]\tLoss: 80.215935\n",
      "Train Epoch: 7 [160/506 (63%)]\tLoss: 847.975220\n",
      "Train Epoch: 7 [200/506 (79%)]\tLoss: 26.650959\n",
      "Train Epoch: 7 [240/506 (94%)]\tLoss: 23.369482\n",
      "Total loss 41898.08031582832\n",
      "Train Epoch: 8 [0/506 (0%)]\tLoss: 60.562981\n",
      "Train Epoch: 8 [40/506 (16%)]\tLoss: 209.874573\n",
      "Train Epoch: 8 [80/506 (31%)]\tLoss: 12.355794\n",
      "Train Epoch: 8 [120/506 (47%)]\tLoss: 77.442390\n",
      "Train Epoch: 8 [160/506 (63%)]\tLoss: 835.325134\n",
      "Train Epoch: 8 [200/506 (79%)]\tLoss: 27.232950\n",
      "Train Epoch: 8 [240/506 (94%)]\tLoss: 23.214157\n",
      "Total loss 41543.816031455994\n",
      "Train Epoch: 9 [0/506 (0%)]\tLoss: 59.125595\n",
      "Train Epoch: 9 [40/506 (16%)]\tLoss: 204.422546\n",
      "Train Epoch: 9 [80/506 (31%)]\tLoss: 10.124138\n",
      "Train Epoch: 9 [120/506 (47%)]\tLoss: 75.284401\n",
      "Train Epoch: 9 [160/506 (63%)]\tLoss: 823.633667\n",
      "Train Epoch: 9 [200/506 (79%)]\tLoss: 27.990343\n",
      "Train Epoch: 9 [240/506 (94%)]\tLoss: 23.109339\n",
      "Total loss 41227.752329945564\n",
      "Train Epoch: 10 [0/506 (0%)]\tLoss: 58.541004\n",
      "Train Epoch: 10 [40/506 (16%)]\tLoss: 198.260193\n",
      "Train Epoch: 10 [80/506 (31%)]\tLoss: 8.229139\n",
      "Train Epoch: 10 [120/506 (47%)]\tLoss: 73.480324\n",
      "Train Epoch: 10 [160/506 (63%)]\tLoss: 812.766296\n",
      "Train Epoch: 10 [200/506 (79%)]\tLoss: 28.891388\n",
      "Train Epoch: 10 [240/506 (94%)]\tLoss: 23.035236\n",
      "Total loss 40944.44996738434\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss_accum = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        model.send(data.location)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model(data)\n",
    "        loss = ((pred - target)**2).sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.get()\n",
    "        loss = loss.get()\n",
    "        \n",
    "        loss_accum += float(loss)\n",
    "        \n",
    "        if batch_idx % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * data.shape[0], len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "            \n",
    "    print('Total loss', loss_accum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!!! - Time to Join the Community!\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways!\n",
    "\n",
    "### Star PySyft on GitHub\n",
    "\n",
    "The easiest way to help our community is just by starring the Repos! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! You can do so by filling out the form at [http://slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "### Join a Code Project!\n",
    "\n",
    "The best way to contribute to our community is to become a code contributor! At any time you can go to PySyft GitHub Issues page and filter for \"Projects\". This will show you all the top level Tickets giving an overview of what projects you can join! If you don't want to join a project, but you would like to do a bit of coding, you can also look for more \"one off\" mini-projects by searching for GitHub issues marked \"good first issue\".\n",
    "\n",
    "- [PySyft Projects](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3AProject)\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Donate\n",
    "\n",
    "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
    "\n",
    "[OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
